{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadineML/MAML-Pytorch/blob/master/ProtoNet_create_data_split_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The purpose of this *.ipynb* is to create *.csv*-files containing filenames and corresponding labels (deducted from the directory structure) in a specified split of a dataset contained in a specified folder to be used as datasets for training and validating in machine learning algorithms.\n",
        "Depending on your implementation of a dataset for your ML algorithm you might be required to move the actual files into a specific directory structure. This notebook does not move the files in any way, it only creates *.csv*-files and directories, if specified."
      ],
      "metadata": {
        "id": "nmOFggjpY9ac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bzyKmmeswG2H"
      },
      "outputs": [],
      "source": [
        "#@title import necessary modules { form-width: \"15%\", display-mode: \"form\" }\n",
        "import torch\n",
        "from torch.utils.data import Dataset, IterableDataset\n",
        "from torchvision import transforms, datasets\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List, TypeVar, Iterable\n",
        "import os\n",
        "import csv\n",
        "import bisect\n",
        "import functools\n",
        "\n",
        "random.seed(222)\n",
        "np.random.seed(222)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH55r_3RxCNG",
        "outputId": "351751f4-a7f3-4a48-cdf7-b8bb6ba856a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive { form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown This block requires you to go through the login process of your Google Account to access Google Drive where your dataset should be stored.\n",
        "from google.colab import drive\n",
        "base_path = '/content/data'\n",
        "drive.mount(base_path, force_remount=True)\n",
        "wd = os.path.join(base_path, \"MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q4Wg7-cdvf_w"
      },
      "outputs": [],
      "source": [
        "#@title CombinedDataset Class  { form-width: \"15%\", display-mode: \"form\" }\n",
        "T_co = TypeVar('T_co', covariant=True)\n",
        "\n",
        "class CombinedDataset(Dataset[T_co]):\n",
        "  # modified version of torch.utils.data.dataset.ConcatDataset\n",
        "    r\"\"\"Dataset as a combination of multiple datasets.\n",
        "\n",
        "    This class is a modified version of torch.utils.data.dataset.ConcatDataset,\n",
        "    which is useful to assemble different existing datasets.\n",
        "\n",
        "    Args:\n",
        "        datasets (sequence): List of datasets to be concatenated\n",
        "    \"\"\"\n",
        "    datasets: List[Dataset[T_co]]\n",
        "    cumulative_sizes: List[int]\n",
        "\n",
        "    @staticmethod\n",
        "    def cumsum(sequence):\n",
        "        r, s = [], 0\n",
        "        for e in sequence:\n",
        "            l = len(e)\n",
        "            r.append(l + s)\n",
        "            s += l\n",
        "        return r\n",
        "    \n",
        "    @staticmethod\n",
        "    def labels_classes(sequence):\n",
        "        label = 0\n",
        "        classes = []\n",
        "        for ds in sequence:\n",
        "          label += len(ds.classes)\n",
        "          classes.append(ds.classes)\n",
        "        return list(range(label)), classes\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def mapped_labels(labels, classes):\n",
        "        map = {}\n",
        "        cidx = 0\n",
        "        max_label_idx = len(labels)\n",
        "        for idx in range(len(classes)):\n",
        "          for idc in range(0,len(classes[idx])):\n",
        "              if cidx >= max_label_idx:\n",
        "                  return map\n",
        "              map.update({labels[cidx] : (classes[idx][idc], idc)})\n",
        "              cidx += 1\n",
        "              \n",
        "        return map\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def mapped_items_per_label(datasets, map):\n",
        "        items_per_label = {}\n",
        "        max_idx = len(map.keys())\n",
        "        idx = 0\n",
        "        offset = 0\n",
        "        offset2 = 0\n",
        "        \n",
        "        for ds in datasets:\n",
        "          labels = [instance[1] for instance in ds.imgs]\n",
        "          for i in range(idx, idx+len(ds.classes)):\n",
        "            items_per_label.update({i : [j+offset2 for j in range(len(labels)) if labels[j] == map.get(i)[1]]})\n",
        "            offset = len(ds.classes)\n",
        "          offset2 += len(labels)\n",
        "          idx += offset\n",
        "          #offset2 -= 1\n",
        "        return items_per_label          \n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def modified_classes(datasets, grouped_classes):\n",
        "        mod_classes = []\n",
        "        for d in range(len(grouped_classes)):\n",
        "            p = os.path.basename(datasets[d].root)\n",
        "            for c in grouped_classes[d]:\n",
        "                mod_classes.append(p+\" / \"+c)\n",
        "        return mod_classes\n",
        "\n",
        "\n",
        "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
        "        super(CombinedDataset, self).__init__()\n",
        "        self.datasets = list(datasets)\n",
        "        assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]\n",
        "        for d in self.datasets:\n",
        "            assert not isinstance(d, IterableDataset), \"CobinedDataset does not support IterableDataset\"\n",
        "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
        "        \n",
        "        #self.imgs, self.possible_labels, self.grouped_classes = self.labels_classes(self.datasets)\n",
        "        self.possible_labels, self.grouped_classes = self.labels_classes(self.datasets)\n",
        "        self.mapped_labels = self.mapped_labels(self.possible_labels, self.grouped_classes)\n",
        "        self.items_per_label = self.mapped_items_per_label(self.datasets, self.mapped_labels)\n",
        "        self.mod_classes = self.modified_classes(self.datasets, self.grouped_classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cumulative_sizes[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if idx < 0:\n",
        "            if -idx > len(self):\n",
        "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
        "            idx = len(self) + idx\n",
        "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
        "        if dataset_idx == 0:\n",
        "            sample_idx = idx\n",
        "        else:\n",
        "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
        "\n",
        "        return (self.datasets[dataset_idx].imgs[sample_idx][0], self.labels[idx])\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F1-r2FmXvxn0"
      },
      "outputs": [],
      "source": [
        "#@title def create_data_csv{ form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown The version **create_data_csv()** saves the relevant data in the form of **rel_path/filename, label**\n",
        "def create_data_csv(save_to_path, _filename, _label):\n",
        "    with open(save_to_path, 'w', newline='') as csvfile:\n",
        "        header_key = ['filename','label']\n",
        "        new_val = csv.DictWriter(csvfile, fieldnames=header_key)\n",
        "        new_val.writeheader()\n",
        "        for idx in range(len(_filename)):\n",
        "            new_val.writerow({'filename': os.path.relpath(_filename[idx], start=drive_path),'label': _label[idx]})\n",
        "\n",
        "#@markdown The version **create_data_csv_1** saves the relevant data in the form of **filename, label**\n",
        "def create_data_csv_1(save_to_path, _filename, _label):\n",
        "    with open(save_to_path, 'w', newline='') as csvfile:\n",
        "        header_key = ['filename','label']\n",
        "        new_val = csv.DictWriter(csvfile, fieldnames=header_key)\n",
        "        new_val.writeheader()\n",
        "        for idx in range(len(_filename)):\n",
        "            new_val.writerow({'filename': os.path.basename(_filename[idx]),'label': _label[idx]})\n",
        "\n",
        "#@markdown By selecting the *keep_rel_path_in_filename checkbox* you switch from using **create_data_csv_1** (default) to using **create_data_csv()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W_xKsdY9uzWV"
      },
      "outputs": [],
      "source": [
        "#@title Provide a path to the dataset { form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown If your dataset consists of multiple folders for different classes with subfolders for their respective labels, please choose CombinedDataset. If you have one folder with one class containing subfolders for lables, choose ImageFolder.\n",
        "dataset_type = \"ImageFolder\" #@param [\"ImageFolder\", \"CombinedDataset\"]\n",
        "#@markdown Please provide a path to your dataset, relative to your Google Drive \"root\" level.\n",
        "rel_data_path = \"images\" #@param {type:\"string\"}\n",
        "#@markdown Please select how to split your dataset into training and validation set\n",
        "max_train_data = 10 #@param {type:\"integer\"}\n",
        "max_val_data = 30 #@param {type:\"integer\"}\n",
        "keep_rel_path_in_filename = True #@param {type:\"boolean\"}\n",
        "#@markdown Please select this checkbox if want to create a folder for the *.csv* files under the **parent directory** of *rel_data_path*\n",
        "create_folder = True #@param {type:\"boolean\"}\n",
        "\n",
        "drive_path = os.path.join(wd, rel_data_path)\n",
        "custom_mode = False\n",
        "if dataset_type == \"CombinedDataset\":\n",
        "    custom_mode = True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lCTD_VX-wBxz"
      },
      "outputs": [],
      "source": [
        "#@title Create datasets { form-width: \"15%\", display-mode: \"form\" }\n",
        "\n",
        "\n",
        "d_transform = transforms.Compose([])\n",
        "\n",
        "if custom_mode:\n",
        "    all_folders = [os.path.join(drive_path,  f) for f in sorted(os.listdir(drive_path))]\n",
        "\n",
        "    #create dataset from all folders in the specified path\n",
        "    test_fp = os.path.join(drive_path, \"test\")\n",
        "    train_fp = os.path.join(drive_path, \"train\")\n",
        "\n",
        "    if train_fp in all_folders and test_fp in all_folders:\n",
        "        folder_list = [os.path.join(train_fp, f) for f in sorted(os.listdir(train_fp))]\n",
        "        folder_list2 = [os.path.join(test_fp, f) for f in sorted(os.listdir(test_fp))]\n",
        "        all_folders.remove(train_fp)\n",
        "        all_folders.remove(test_fp)\n",
        "        all_folders.extend(folder_list)\n",
        "        all_folders.extend(folder_list2)\n",
        "        \n",
        "    \n",
        "    dataset_list = []\n",
        "    \n",
        "    \n",
        "    for folder in all_folders:\n",
        "        _ds = datasets.ImageFolder(root=folder, transform=d_transform)\n",
        "        dataset_list.append(_ds)\n",
        "\n",
        "    ds = CombinedDataset(dataset_list)\n",
        "\n",
        "    \n",
        "    _filename = []\n",
        "    _label = []\n",
        "    for im in range(len(ds)):\n",
        "        _filename.append(ds[im][0])\n",
        "        _label.append(ds.mod_classes[ds[im][1]])\n",
        "\n",
        "    items_per_label = ds.items_per_label\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "    ds = datasets.ImageFolder(root=drive_path, transform=d_transform)\n",
        "\n",
        "    items_per_label = {}\n",
        "    max_idx = len(ds.classes)\n",
        "    labels = ds.targets\n",
        "    for i in range(max_idx):\n",
        "        items_per_label.update({i : [j for j in range(len(labels)) if labels[j] == i]})\n",
        "\n",
        "\n",
        "    \n",
        "    _filename = []\n",
        "    _label = []\n",
        "    for im in range(len(ds)):\n",
        "        _filename.append(ds.imgs[im][0])\n",
        "        _label.append(ds.classes[ds.imgs[im][1]])\n",
        "\n",
        "lens = []\n",
        "for l in items_per_label.keys():\n",
        "    lens.append(len(items_per_label[l]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "afKxNh_B0wF2"
      },
      "outputs": [],
      "source": [
        "#@title Split datasets { form-width: \"15%\", display-mode: \"form\" }\n",
        "random.seed(222)\n",
        "np.random.seed(222)\n",
        "\n",
        "_val_filename = []\n",
        "val_label = []\n",
        "#_test_filename = []\n",
        "#test_label = [] \n",
        "_train_filename = []\n",
        "train_label = []\n",
        "\n",
        "for idx in range(len(lens)):\n",
        "    l = lens[idx]\n",
        "    k = min(l, max_train_data)\n",
        "    m = min(l-k, max_val_data)\n",
        "    cur_class = ds.classes[idx]\n",
        "\n",
        "    data = items_per_label[idx]\n",
        "    np.random.shuffle(data)\n",
        "    train_slice = data[:k]\n",
        "    #test_slice = data[k:k+m]\n",
        "    val_slice = data[k:k+m]\n",
        "\n",
        "    \n",
        "    _train_filename.extend(train_slice)\n",
        "    train_label.extend([cur_class]*k)\n",
        "    #_test_filename.extend(test_slice)\n",
        "    #test_label.extend([cur_class]*m)\n",
        "    _val_filename.extend(val_slice)\n",
        "    val_label.extend([cur_class]*m)\n",
        "\n",
        "val_filename = [_filename[i] for i in _val_filename]\n",
        "#test_filename = [_filename[i] for i in _test_filename]\n",
        "train_filename = [_filename[i] for i in _train_filename]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nvCWmoMmP2yj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a980e5-b6bd-4abb-94e4-da585ec7e48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 2 CSV files at \"/content/data/MyDrive/ProtoNet: images 10-30\".\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Save *.csv files { form-width: \"15%\", display-mode: \"form\" }\n",
        "if create_folder:\n",
        "    d = os.path.dirname(drive_path)\n",
        "    b = os.path.basename(drive_path)\n",
        "    save_to = os.path.join(d, \"ProtoNet: \"+b+\" {}-{}\".format(max_train_data, max_val_data))\n",
        "\n",
        "    os.makedirs(save_to, exist_ok=True)\n",
        "else:\n",
        "    save_to = drive_path\n",
        "\n",
        "all_sample_path = os.path.join(save_to, \"all_samples.csv\")\n",
        "val_sample_path = os.path.join(save_to, \"val.csv\")\n",
        "train_sample_path = os.path.join(save_to, \"train.csv\")\n",
        "#test_sample_path = os.path.join(save_to, \"test.csv\")\n",
        "\n",
        "if keep_rel_path_in_filename:\n",
        "    create_data_csv(all_sample_path, _filename, _label)\n",
        "    create_data_csv(val_sample_path, val_filename, val_label)\n",
        "    #create_data_csv(test_sample_path, test_filename, test_label)\n",
        "    create_data_csv(train_sample_path, train_filename, train_label)\n",
        "\n",
        "else:\n",
        "    create_data_csv_1(all_sample_path, _filename, _label)\n",
        "    create_data_csv_1(val_sample_path, val_filename, val_label)\n",
        "    #create_data_csv_1(test_sample_path, test_filename, test_label)\n",
        "    create_data_csv_1(train_sample_path, train_filename, train_label)\n",
        "\n",
        "print(f\"Created 2 CSV files at \\\"{save_to}\\\".\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ProtoNet create data  split csv.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}